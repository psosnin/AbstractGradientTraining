{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import model_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda:0\")\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 20\n",
    "pretrain_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2], balanced=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3892008/122059171.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\".models/medmnist.ckpt\"):\n",
    "    model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\", )\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    # save the model\n",
    "    with open(\".models/medmnist.ckpt\", \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model on the private Drusen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "batchsize = 3000\n",
    "config = AGTConfig(\n",
    "    fragsize=1000,\n",
    "    learning_rate=0.1,\n",
    "    n_epochs=2,\n",
    "    k_unlearn=50,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:0\",\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    lr_decay=0.3,\n",
    "    lr_min=0.001,\n",
    "    log_level=\"DEBUG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_other = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[2])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(batchsize, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.46\n",
      "Classes 0, 1, 3  : nominal = 0.96\n",
      "All Classes      : nominal = 0.84\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pre-trained model\n",
    "conv_layers = model[0:5]\n",
    "linear_layers = model[5:-1]\n",
    "conv_transform = model_utils.get_conv_model_transform(conv_layers)\n",
    "param_l, param_n, param_u = model_utils.get_parameters(linear_layers)\n",
    "\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "other_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_other)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [17:00:47] =================== Starting Unlearning Certified Training ===================\n",
      "[AGT] [DEBUG   ] [17:00:47] \tOptimizer params: n_epochs=2, learning_rate=0.1, l1_reg=0.0, l2_reg=0.0\n",
      "[AGT] [DEBUG   ] [17:00:47] \tLearning rate schedule: lr_decay=0.3, lr_min=0.001, early_stopping=True\n",
      "[AGT] [DEBUG   ] [17:00:47] \tUnlearning parameter: k_unlearn=50\n",
      "[AGT] [DEBUG   ] [17:00:47] \tClipping: gamma=inf, method=clamp\n",
      "[AGT] [DEBUG   ] [17:00:47] \tNoise: type=gaussian, sigma=0\n",
      "[AGT] [DEBUG   ] [17:00:47] \tBounding methods: forward=interval, loss=binary_cross_entropy, backward=interval\n",
      "[AGT] [INFO    ] [17:00:47] Starting epoch 1\n",
      "[AGT] [DEBUG   ] [17:00:47] Initialising dataloader batchsize to 3000\n",
      "[AGT] [INFO    ] [17:00:47] Training batch 1: Network eval bounds=(0.46, 0.46, 0.46), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [17:00:47] Training batch 2: Network eval bounds=(0.72, 0.76, 0.76), W0 Bound=0.0748 \n",
      "[AGT] [INFO    ] [17:00:48] Training batch 3: Network eval bounds=(0.76, 0.84, 0.86), W0 Bound=0.142 \n",
      "[AGT] [INFO    ] [17:00:49] Training batch 4: Network eval bounds=(0.72, 0.84, 0.92), W0 Bound=0.218 \n",
      "[AGT] [INFO    ] [17:00:49] Training batch 5: Network eval bounds=(0.66, 0.84, 0.94), W0 Bound=0.304 \n",
      "[AGT] [DEBUG   ] [17:00:50] Skipping batch 6 in epoch 1 (expected batchsize 3000, got 508)\n",
      "[AGT] [INFO    ] [17:00:50] Starting epoch 2\n",
      "[AGT] [INFO    ] [17:00:50] Training batch 6: Network eval bounds=(0.55, 0.85, 0.98), W0 Bound=0.404 \n",
      "[AGT] [INFO    ] [17:00:50] Training batch 7: Network eval bounds=(0.42, 0.85, 0.99), W0 Bound=0.525 \n",
      "[AGT] [INFO    ] [17:00:51] Training batch 8: Network eval bounds=(0.31, 0.85, 1   ), W0 Bound=0.659 \n",
      "[AGT] [INFO    ] [17:00:52] Training batch 9: Network eval bounds=(0.22, 0.85, 1   ), W0 Bound=0.805 \n",
      "[AGT] [INFO    ] [17:00:52] Training batch 10: Network eval bounds=(0.16, 0.86, 1   ), W0 Bound=0.957 \n",
      "[AGT] [DEBUG   ] [17:00:53] Skipping batch 6 in epoch 2 (expected batchsize 3000, got 508)\n",
      "[AGT] [INFO    ] [17:00:53] Final network eval: Network eval bounds=(0.11, 0.86, 1   ), W0 Bound=1.11 \n",
      "[AGT] [INFO    ] [17:00:53] =================== Finished Unlearning Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = agt.unlearning_certified_training(\n",
    "    linear_layers, config, dl_train, dl_test_drusen, transform=conv_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.86, certified bound = 0.11\n",
      "Classes 0, 1, 3  : nominal = 0.88, certified bound = 0.48\n",
      "All Classes      : nominal = 0.88, certified bound = 0.39\n"
     ]
    }
   ],
   "source": [
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform)\n",
    "other_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_other)), transform=conv_transform)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}, certified bound = {other_acc[0]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import model_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist\n",
    "from models.robust_regularizer import parameter_gradient_interval_regularizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the robustness of a non-robustly pre-trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3890792/1391520775.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  standard_model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of non-robustly trained classifier on test set with epsilon=0.01: [0.00, 0.96, 1.00]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:1\")\n",
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "standard_model = DeepMindSmall(1, 1).to(device)\n",
    "standard_model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "params_l, params_n, params_u = model_utils.get_parameters(standard_model[5:-1])\n",
    "epsilon = 0.01\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l,\n",
    "    params_n,\n",
    "    params_u,\n",
    "    test_batch,\n",
    "    test_labels,\n",
    "    transform=model_utils.get_conv_model_transform(standard_model[0:5]),\n",
    "    epsilon=epsilon,\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "\n",
    "print(f\"Accuracy of non-robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(1)\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 10\n",
    "pretrain_learning_rate = 0.001\n",
    "pretrain_epsilon = 0.55\n",
    "pretrain_model_epsilon = 0.001\n",
    "pretrain_reg_strength = 0.4\n",
    "model_path = f\".models/medmnist_robust_eps{pretrain_epsilon}_alpha{pretrain_reg_strength}_meps{pretrain_model_epsilon}.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2], balanced=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3890792/268843238.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\")\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            bce_loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            regularization = parameter_gradient_interval_regularizer(\n",
    "                model, x, u, \"binary_cross_entropy\", pretrain_epsilon, pretrain_model_epsilon\n",
    "            )\n",
    "            loss = bce_loss + pretrain_reg_strength * regularization\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item(), bce_loss=bce_loss.item(), reg=regularization.item())\n",
    "    # save the model\n",
    "    with open(model_path, \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the robustness of the model pre-trained with the gradient interval regularization term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".models/medmnist_robust_eps0.55_alpha0.4_meps0.001.ckpt 0.32, 0.87, 1.00\n",
      "Accuracy of robustly trained classifier on test set with epsilon=0.01: [0.32, 0.87, 1.00]\n"
     ]
    }
   ],
   "source": [
    "conv_layers = model[0:5]\n",
    "linear_layers = model[5:-1]\n",
    "conv_transform = model_utils.get_conv_model_transform(conv_layers)\n",
    "params_l, params_n, params_u = model_utils.get_parameters(linear_layers)\n",
    "\n",
    "_, dl_test = oct_mnist.get_dataloaders(1000, exclude_classes=[2], balanced=True)\n",
    "test_batch, test_labels = next(iter(dl_test))\n",
    "accs = agt.test_metrics.test_accuracy(\n",
    "    params_l,\n",
    "    params_n,\n",
    "    params_u,\n",
    "    test_batch,\n",
    "    test_labels,\n",
    "    transform=conv_transform,\n",
    "    epsilon=0.01,\n",
    ")\n",
    "accs = \", \".join([f\"{a:.2f}\" for a in accs])\n",
    "print(model_path, accs)\n",
    "print(f\"Accuracy of robustly trained classifier on test set with epsilon={epsilon}: [{accs}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Include all classes, only allowing class 2 (Drusen) to be potentially poisoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "clean_batchsize = 3000\n",
    "drusen_batchsize = 3000\n",
    "test_batchsize = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.51\n",
      "Classes 0, 1, 3  : nominal = 0.85\n",
      "All Classes      : nominal = 0.77\n",
      "[AGT] [INFO    ] [16:55:46] =================== Starting Poison Certified Training ===================\n",
      "[AGT] [DEBUG   ] [16:55:46] \tOptimizer params: n_epochs=2, learning_rate=0.06, l1_reg=0.0, l2_reg=0.0\n",
      "[AGT] [DEBUG   ] [16:55:46] \tLearning rate schedule: lr_decay=4.0, lr_min=0.001, early_stopping=True\n",
      "[AGT] [DEBUG   ] [16:55:46] \tAdversary feature-space budget: epsilon=0.01, k_poison=50\n",
      "[AGT] [DEBUG   ] [16:55:46] \tAdversary label-space budget: label_epsilon=0, label_k_poison=0, poison_target=-1\n",
      "[AGT] [DEBUG   ] [16:55:46] \tClipping: gamma=inf, method=clamp\n",
      "[AGT] [DEBUG   ] [16:55:46] \tBounding methods: forward=interval, loss=binary_cross_entropy, backward=interval\n",
      "[AGT] [INFO    ] [16:55:46] Starting epoch 1\n",
      "[AGT] [DEBUG   ] [16:55:46] Initialising dataloader batchsize to 6000\n",
      "[AGT] [INFO    ] [16:55:46] Training batch 1: Network eval bounds=(0.51, 0.51, 0.51), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [16:55:47] Training batch 2: Network eval bounds=(0.87, 0.89, 0.89), W0 Bound=0.0135 \n",
      "[AGT] [DEBUG   ] [16:55:48] Skipping batch 3 in epoch 1 (expected batchsize 6000, got 4754)\n",
      "[AGT] [INFO    ] [16:55:48] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:55:48] Training batch 3: Network eval bounds=(0.88, 0.9 , 0.92), W0 Bound=0.0177 \n",
      "[AGT] [INFO    ] [16:55:49] Training batch 4: Network eval bounds=(0.89, 0.91, 0.93), W0 Bound=0.0204 \n",
      "[AGT] [DEBUG   ] [16:55:50] Skipping batch 3 in epoch 2 (expected batchsize 6000, got 4754)\n",
      "[AGT] [INFO    ] [16:55:50] Final network eval: Network eval bounds=(0.89, 0.91, 0.93), W0 Bound=0.0225 \n",
      "[AGT] [INFO    ] [16:55:50] =================== Finished Poison Certified Training ===================\n",
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.91, certified bound = 0.89\n",
      "Classes 0, 1, 3  : nominal = 0.8, certified bound = 0.78\n",
      "All Classes      : nominal = 0.83, certified bound = 0.81\n"
     ]
    }
   ],
   "source": [
    "from abstract_gradient_training.poisoning import poison_certified_training\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# get dataloaders\n",
    "dl_clean, dl_test_clean = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize, exclude_classes=[2])\n",
    "dl_drusen, dl_test_drusen = oct_mnist.get_dataloaders(drusen_batchsize, test_batchsize, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize)\n",
    "\n",
    "# evaluate the pre-trained model\n",
    "param_l, param_n, param_u = model_utils.get_parameters(linear_layers)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "clean_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\", file=sys.stderr)\n",
    "\n",
    "config = AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=0.06,\n",
    "    n_epochs=2,\n",
    "    k_poison=50,\n",
    "    epsilon=0.01,\n",
    "    # clip_gamma = 2.0,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:1\",\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    log_level=\"DEBUG\",\n",
    "    lr_decay=4.0,\n",
    "    lr_min=0.001,\n",
    ")\n",
    "\n",
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = poison_certified_training(\n",
    "    linear_layers, config, dl_drusen, dl_test_drusen, dl_clean=dl_clean, transform=conv_transform\n",
    ")\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "clean_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform\n",
    ")\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}, certified bound = {clean_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\", file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.51\n",
      "Classes 0, 1, 3  : nominal = 0.85\n",
      "All Classes      : nominal = 0.77\n",
      "[AGT] [INFO    ] [16:56:29] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [16:56:29] Starting epoch 1\n",
      "[AGT] [INFO    ] [16:56:29] Training batch 1: Network eval bounds=(0.51, 0.51, 0.51), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [16:56:30] Training batch 2: Network eval bounds=(0.68, 0.94, 0.99), W0 Bound=3.77 \n",
      "[AGT] [INFO    ] [16:56:31] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:56:31] Training batch 3: Network eval bounds=(0.51, 0.94, 1   ), W0 Bound=4.53 \n",
      "[AGT] [INFO    ] [16:56:32] Training batch 4: Network eval bounds=(0.36, 0.94, 1   ), W0 Bound=4.95 \n",
      "[AGT] [INFO    ] [16:56:33] Final network eval: Network eval bounds=(0.24, 0.94, 1   ), W0 Bound=5.24 \n",
      "[AGT] [INFO    ] [16:56:33] =================== Finished Privacy Certified Training ===================\n",
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.94, certified bound = 0.24\n",
      "Classes 0, 1, 3  : nominal = 0.76, certified bound = 0.43\n",
      "All Classes      : nominal = 0.81, certified bound = 0.38\n"
     ]
    }
   ],
   "source": [
    "from abstract_gradient_training.privacy import privacy_certified_training\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# get dataloaders\n",
    "dl_clean, dl_test_clean = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize, exclude_classes=[2])\n",
    "dl_drusen, dl_test_drusen = oct_mnist.get_dataloaders(drusen_batchsize, test_batchsize, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize)\n",
    "\n",
    "# evaluate the pre-trained model\n",
    "param_l, param_n, param_u = model_utils.get_parameters(linear_layers)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "clean_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform\n",
    ")\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\", file=sys.stderr)\n",
    "\n",
    "config = AGTConfig(\n",
    "    fragsize=500,\n",
    "    learning_rate=0.08,\n",
    "    n_epochs=2,\n",
    "    k_private=50,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:0\",\n",
    "    clip_gamma=5.0,\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    lr_decay=4.0,\n",
    "    lr_min=0.001,\n",
    ")\n",
    "\n",
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = privacy_certified_training(\n",
    "    linear_layers, config, dl_drusen, dl_test_drusen, dl_public=dl_clean, transform=conv_transform\n",
    ")\n",
    "\n",
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "clean_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform\n",
    ")\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\", file=sys.stderr)\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}, certified bound = {clean_acc[0]:.2g}\", file=sys.stderr)\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\", file=sys.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

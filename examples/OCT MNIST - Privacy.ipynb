{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import model_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(1)\n",
    "device = torch.device(\"cuda:1\")\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 20\n",
    "pretrain_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2], balanced=True)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3889396/122059171.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\".models/medmnist.ckpt\"):\n",
    "    model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\", )\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    # save the model\n",
    "    with open(\".models/medmnist.ckpt\", \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model on the private Drusen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "batchsize = 10000\n",
    "config = AGTConfig(\n",
    "    fragsize=100,\n",
    "    learning_rate=0.1,\n",
    "    n_epochs=2,\n",
    "    k_private=10,\n",
    "    forward_bound=\"interval\",\n",
    "    device=\"cuda:1\",\n",
    "    backward_bound=\"interval\",\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    clip_gamma=2.0,\n",
    "    # noise_multiplier=0.1,\n",
    "    lr_decay=1.0,\n",
    "    lr_min=0.001,\n",
    "    log_level=\"DEBUG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dl_train, _ = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1], balanced=True)\n",
    "_, dl_test_drusen = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_other = oct_mnist.get_dataloaders(batchsize, 1000, exclude_classes=[2])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(batchsize, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = model[0:5]\n",
    "linear_layers = model[5:-1]\n",
    "conv_transform = model_utils.get_conv_model_transform(conv_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.46\n",
      "Classes 0, 1, 3  : nominal = 0.96\n",
      "All Classes      : nominal = 0.84\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pre-trained model\n",
    "param_l, param_n, param_u = model_utils.get_parameters(linear_layers)\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform)\n",
    "other_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_other)), transform=conv_transform)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [16:46:32] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [DEBUG   ] [16:46:32] \tOptimizer params: n_epochs=2, learning_rate=0.1, l1_reg=0.0, l2_reg=0.0\n",
      "[AGT] [DEBUG   ] [16:46:32] \tLearning rate schedule: lr_decay=1.0, lr_min=0.001, early_stopping=True\n",
      "[AGT] [DEBUG   ] [16:46:32] \tPrivacy parameter: k_private=10\n",
      "[AGT] [DEBUG   ] [16:46:32] \tClipping: gamma=2.0, method=clamp\n",
      "[AGT] [DEBUG   ] [16:46:32] \tNoise: type=gaussian, sigma=0\n",
      "[AGT] [DEBUG   ] [16:46:32] \tBounding methods: forward=interval, loss=binary_cross_entropy, backward=interval\n",
      "[AGT] [INFO    ] [16:46:32] Starting epoch 1\n",
      "[AGT] [DEBUG   ] [16:46:33] Initialising dataloader batchsize to 10000\n",
      "[AGT] [INFO    ] [16:46:33] Training batch 1: Network eval bounds=(0.46, 0.46, 0.46), W0 Bound=0.0 \n",
      "[AGT] [DEBUG   ] [16:46:36] Skipping batch 2 in epoch 1 (expected batchsize 10000, got 5508)\n",
      "[AGT] [WARNING ] [16:46:36] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [16:46:36] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:46:36] Training batch 2: Network eval bounds=(0.67, 0.68, 0.71), W0 Bound=0.231 \n",
      "[AGT] [DEBUG   ] [16:46:39] Skipping batch 2 in epoch 2 (expected batchsize 10000, got 5508)\n",
      "[AGT] [WARNING ] [16:46:39] Dataloader has only one batch per epoch, effective batchsize may be smaller than expected.\n",
      "[AGT] [INFO    ] [16:46:39] Final network eval: Network eval bounds=(0.72, 0.76, 0.79), W0 Bound=0.35 \n",
      "[AGT] [INFO    ] [16:46:39] =================== Finished Privacy Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = agt.privacy_certified_training(\n",
    "    linear_layers, config, dl_train, dl_test_drusen, transform=conv_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.76, certified bound = 0.72\n",
      "Classes 0, 1, 3  : nominal = 0.92, certified bound = 0.92\n",
      "All Classes      : nominal = 0.88, certified bound = 0.87\n",
      "======= Percentage of points with certified unlearning-safe guarantees =========\n",
      "Class 2 (Drusen) : 0.93\n",
      "Classes 0, 1, 3  : 0.99\n",
      "All Classes      : 0.97\n"
     ]
    }
   ],
   "source": [
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "other_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_other)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc[1]:.2g}, certified bound = {other_acc[0]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\")\n",
    "\n",
    "# percentage of points with a certified unlearning-safe guarantee\n",
    "percent_certified_all = agt.test_metrics.proportion_certified(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform\n",
    ")\n",
    "percent_certified_drusen = agt.test_metrics.proportion_certified(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "percent_certified_other = agt.test_metrics.proportion_certified(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_other)), transform=conv_transform\n",
    ")\n",
    "print(f\"======= Percentage of points with certified unlearning-safe guarantees =========\")\n",
    "print(f\"Class 2 (Drusen) : {percent_certified_drusen:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : {percent_certified_other:.2g}\")\n",
    "print(f\"All Classes      : {percent_certified_all:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.74, certified bound = 0.74\n",
      "Classes 0, 1, 3  : nominal = 0.89, certified bound = 0.89\n",
      "All Classes      : nominal = 0.85, certified bound = 0.85\n"
     ]
    }
   ],
   "source": [
    "# make private predictions\n",
    "noise_level = 1.0 / 5.0\n",
    "drusen_acc = agt.privacy_utils.noisy_test_accuracy(\n",
    "    param_n, *next(iter(dl_test_drusen)), transform=conv_transform, noise_level=noise_level\n",
    ")\n",
    "other_acc = agt.privacy_utils.noisy_test_accuracy(\n",
    "    param_n, *next(iter(dl_test_other)), transform=conv_transform, noise_level=noise_level\n",
    ")\n",
    "all_acc = agt.privacy_utils.noisy_test_accuracy(\n",
    "    param_n, *next(iter(dl_test_all)), transform=conv_transform, noise_level=noise_level\n",
    ")\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc:.2g}, certified bound = {drusen_acc:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {other_acc:.2g}, certified bound = {other_acc:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc:.2g}, certified bound = {all_acc:.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

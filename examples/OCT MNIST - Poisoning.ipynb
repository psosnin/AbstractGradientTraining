{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Finetuning of a Classifier on the OCT-MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training import model_utils\n",
    "from models.deepmind import DeepMindSmall \n",
    "from datasets import oct_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-train the model\n",
    "\n",
    "Exclude class 2 (Drusen) from the pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pre-training\n",
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cuda:0\")\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 20\n",
    "pretrain_learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model, dataset and optimizer\n",
    "model = DeepMindSmall(1, 1)\n",
    "dl_pretrain, _ = oct_mnist.get_dataloaders(pretrain_batchsize, exclude_classes=[2])\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3884527/122059171.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\".models/medmnist.ckpt\"):\n",
    "    model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\", )\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    # save the model\n",
    "    with open(\".models/medmnist.ckpt\", \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the model\n",
    "\n",
    "Include all classes, only allowing class 2 (Drusen) to be potentially poisoned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up fine-tuning parameters\n",
    "clean_batchsize = 7000\n",
    "drusen_batchsize = 3000\n",
    "test_batchsize = 1000\n",
    "\n",
    "config = AGTConfig(\n",
    "    fragsize = 2000,\n",
    "    learning_rate = 0.6,\n",
    "    n_epochs = 2,\n",
    "    k_poison = 50,\n",
    "    epsilon = 0.01,\n",
    "    forward_bound = \"interval\",\n",
    "    device = \"cuda:0\",\n",
    "    backward_bound = \"interval\",\n",
    "    loss = \"binary_cross_entropy\",\n",
    "    lr_decay=2.0,\n",
    "    lr_min=0.001,\n",
    "    log_level=\"DEBUG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders\n",
    "dl_clean, dl_test_clean = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize, exclude_classes=[2])\n",
    "dl_drusen, dl_test_drusen = oct_mnist.get_dataloaders(drusen_batchsize, test_batchsize, exclude_classes=[0, 1, 3])\n",
    "_, dl_test_all = oct_mnist.get_dataloaders(clean_batchsize, test_batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Pre-trained model accuracy ===========\n",
      "Class 2 (Drusen) : nominal = 0.46\n",
      "Classes 0, 1, 3  : nominal = 0.96\n",
      "All Classes      : nominal = 0.84\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pre-trained model\n",
    "conv_layers = model[0:5]\n",
    "linear_layers = model[5:-1]\n",
    "conv_transform = model_utils.get_conv_model_transform(conv_layers)\n",
    "param_l, param_n, param_u = model_utils.get_parameters(linear_layers)\n",
    "\n",
    "drusen_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform\n",
    ")\n",
    "clean_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform\n",
    ")\n",
    "all_acc = agt.test_metrics.test_accuracy(\n",
    "    param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform\n",
    ")\n",
    "\n",
    "print(\"=========== Pre-trained model accuracy ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [16:58:21] =================== Starting Poison Certified Training ===================\n",
      "[AGT] [DEBUG   ] [16:58:21] \tOptimizer params: n_epochs=2, learning_rate=0.6, l1_reg=0.0, l2_reg=0.0\n",
      "[AGT] [DEBUG   ] [16:58:21] \tLearning rate schedule: lr_decay=2.0, lr_min=0.001, early_stopping=True\n",
      "[AGT] [DEBUG   ] [16:58:21] \tAdversary feature-space budget: epsilon=0.01, k_poison=50\n",
      "[AGT] [DEBUG   ] [16:58:21] \tAdversary label-space budget: label_epsilon=0, label_k_poison=0, poison_target=-1\n",
      "[AGT] [DEBUG   ] [16:58:21] \tClipping: gamma=inf, method=clamp\n",
      "[AGT] [DEBUG   ] [16:58:21] \tBounding methods: forward=interval, loss=binary_cross_entropy, backward=interval\n",
      "[AGT] [INFO    ] [16:58:21] Starting epoch 1\n",
      "[AGT] [DEBUG   ] [16:58:22] Initialising dataloader batchsize to 10000\n",
      "[AGT] [INFO    ] [16:58:22] Training batch 1: Network eval bounds=(0.46, 0.46, 0.46), W0 Bound=0.0 \n",
      "[AGT] [INFO    ] [16:58:23] Training batch 2: Network eval bounds=(0.97, 1   , 1   ), W0 Bound=1.15 \n",
      "[AGT] [DEBUG   ] [16:58:24] Skipping batch 3 in epoch 1 (expected batchsize 10000, got 8754)\n",
      "[AGT] [INFO    ] [16:58:24] Starting epoch 2\n",
      "[AGT] [INFO    ] [16:58:24] Training batch 3: Network eval bounds=(0.23, 0.89, 1   ), W0 Bound=1.82 \n",
      "[AGT] [WARNING ] [16:58:26] Early stopping due to loose bounds\n",
      "[AGT] [INFO    ] [16:58:26] Final network eval: Network eval bounds=(0.028, 0.85, 1   ), W0 Bound=2.61 \n",
      "[AGT] [INFO    ] [16:58:26] =================== Finished Poison Certified Training ===================\n"
     ]
    }
   ],
   "source": [
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "param_l, param_n, param_u = agt.poison_certified_training(\n",
    "    linear_layers, config, dl_drusen, dl_test_drusen, dl_clean=dl_clean, transform=conv_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Fine-tuned model accuracy + bounds ===========\n",
      "Class 2 (Drusen) : nominal = 0.85, certified bound = 0.028\n",
      "Classes 0, 1, 3  : nominal = 0.88, certified bound = 0.25\n",
      "All Classes      : nominal = 0.88, certified bound = 0.2\n"
     ]
    }
   ],
   "source": [
    "# evaluate the fine-tuned model\n",
    "drusen_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_drusen)), transform=conv_transform)\n",
    "clean_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_clean)), transform=conv_transform)\n",
    "all_acc = agt.test_metrics.test_accuracy(param_l, param_n, param_u, *next(iter(dl_test_all)), transform=conv_transform)\n",
    "\n",
    "print(\"=========== Fine-tuned model accuracy + bounds ===========\")\n",
    "print(f\"Class 2 (Drusen) : nominal = {drusen_acc[1]:.2g}, certified bound = {drusen_acc[0]:.2g}\")\n",
    "print(f\"Classes 0, 1, 3  : nominal = {clean_acc[1]:.2g}, certified bound = {clean_acc[0]:.2g}\")\n",
    "print(f\"All Classes      : nominal = {all_acc[1]:.2g}, certified bound = {all_acc[0]:.2g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Certified Finetuning of a Classifier on the OCT-MNIST Dataset\n",
    "\n",
    "In this notebook, we'll fine-tune a classifier on the OCT-MNIST dataset using Abstract Gradient Training. The OCT-MNIST dataset is a medical imaging dataset that contains 4 diagnostic classes (choroidal neovascularization, diabetic macular edem, drusen, and normal retina). We'll tackle the binary-classification problem of distinguishing between normal retina and the other classes.\n",
    "\n",
    "We'll assume the following setting:\n",
    "\n",
    "- The model is first pre-trained on the dataset with the drusen class removed, i.e. trained to distinguish normal vs. (choroidal neovascularization, diabetic macular edem) classes.\n",
    "- We'll then fine-tune the model on the full dataset (including the drusen class) using Abstract Gradient Training for privacy-safe certification.\n",
    "- The model is a convolutional network with 3 convolutional layers and 2 fully connected layers. The pre-training will train all the layers, while the fine-tuning only trains the dense layers using AGT.\n",
    "\n",
    "This is simulating a setting in which a model is pre-trained on public data and then fine-tuned on private / sensitive data (in this case the drusen class). We'll then use the certificates provided by AGT to make privacy-preserving predictions:\n",
    "\n",
    "- Running AGT for a range of k_private values, we can use the resulting parameter bounds to compute a bound on the smooth sensitivity of the model for a given prediction.\n",
    "- Using the smooth sensitivity bounds, we can calibrate the noise to add to the prediction to ensure differential privacy for a given epsilon.\n",
    "- We can then use the calibrated noise to make privacy-preserving predictions, which should maintain high utility when compared with noise calibrated to the global sensitivity. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "import torchvision\n",
    "\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training import AGTConfig\n",
    "from abstract_gradient_training.bounded_models import IntervalBoundedModel\n",
    "\n",
    "from medmnist import OCTMNIST  # python -m pip install git+https://github.com/MedMNIST/MedMNIST.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(exclude_classes=None, balanced=False):\n",
    "    \"\"\"\n",
    "    Get OCT MedMNIST dataset as a binary classification problem of class 3 (normal) vs classes 0, 1, 2.\n",
    "    \"\"\"\n",
    "\n",
    "    # get the datasets\n",
    "    train_dataset = OCTMNIST(split=\"train\", transform=torchvision.transforms.ToTensor())\n",
    "    test_dataset = OCTMNIST(split=\"test\", transform=torchvision.transforms.ToTensor())\n",
    "    train_imgs, train_labels = train_dataset.imgs, train_dataset.labels\n",
    "    test_imgs, test_labels = test_dataset.imgs, test_dataset.labels\n",
    "\n",
    "    # filter out excluded classes\n",
    "    if exclude_classes is not None:\n",
    "        for e in exclude_classes:\n",
    "            train_imgs = train_imgs[(train_labels != e).squeeze()]\n",
    "            train_labels = train_labels[(train_labels != e).squeeze()]\n",
    "            test_imgs = test_imgs[(test_labels != e).squeeze()]\n",
    "            test_labels = test_labels[(test_labels != e).squeeze()]\n",
    "\n",
    "    # convert to a binary classification problem\n",
    "    train_labels = train_labels != 3  # i.e. 0 = normal, 1 = abnormal\n",
    "    test_labels = test_labels != 3\n",
    "\n",
    "    # apply the appropriate scaling and transposition\n",
    "    train_imgs = torch.tensor(train_imgs, dtype=torch.float32).unsqueeze(1) / 255\n",
    "    test_imgs = torch.tensor(test_imgs, dtype=torch.float32).unsqueeze(1) / 255\n",
    "    train_labels = torch.tensor(train_labels, dtype=torch.int64)\n",
    "    test_labels = torch.tensor(test_labels, dtype=torch.int64)\n",
    "\n",
    "    # balance the training dataset such that the number of samples in each class is equal\n",
    "    if balanced:\n",
    "        n_ones = train_labels.sum().item()\n",
    "        n_zeros = len(train_labels) - n_ones\n",
    "        n_samples = min(n_ones, n_zeros)\n",
    "        # find the indices of the ones, and then randomly sample n_samples from them\n",
    "        idx_ones = torch.where(train_labels == 1)[0]\n",
    "        ones_selection = torch.randperm(n_ones)\n",
    "        idx_ones = idx_ones[ones_selection][:n_samples]\n",
    "        # find the indices of the zeros, and then randomly sample n_samples from them\n",
    "        idx_zeros = torch.where(train_labels == 0)[0]\n",
    "        zeros_selection = torch.randperm(n_zeros)\n",
    "        idx_zeros = idx_zeros[zeros_selection][:n_samples]\n",
    "        idx = torch.cat([idx_ones, idx_zeros])\n",
    "        train_imgs, train_labels = train_imgs[idx], train_labels[idx]\n",
    "    \n",
    "    # form dataloaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(train_imgs, train_labels)\n",
    "    test_dataset = torch.utils.data.TensorDataset(test_imgs, test_labels)\n",
    "    return train_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "# small architecture from https://arxiv.org/abs/1810.12715\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(1, 16, 4, 2, 0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(16, 32, 4, 1, 0),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(3200, 100),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(100, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-train the model without the drusen class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the pre-training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pretrain_batchsize = 100\n",
    "pretrain_n_epochs = 20\n",
    "pretrain_learning_rate = 0.001\n",
    "dataset_pretrain, _ = get_datasets(exclude_classes=[2], balanced=True)\n",
    "dl_pretrain = torch.utils.data.DataLoader(dataset_pretrain, batch_size=pretrain_batchsize, shuffle=True)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=pretrain_learning_rate)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26418/2675574202.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n"
     ]
    }
   ],
   "source": [
    "models_dir = \".models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "# check if a pre-trained model exists\n",
    "if os.path.exists(\".models/medmnist.ckpt\"):\n",
    "    model.load_state_dict(torch.load(\".models/medmnist.ckpt\"))\n",
    "else:  # pre-train the model\n",
    "    progress_bar = tqdm.trange(pretrain_n_epochs, desc=\"Epoch\", )\n",
    "    for epoch in progress_bar:\n",
    "        for i, (x, u) in enumerate(dl_pretrain):\n",
    "            # Forward pass\n",
    "            u, x = u.to(device), x.to(device)\n",
    "            output = model(x)\n",
    "            loss = criterion(output.squeeze().float(), u.squeeze().float())\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % 100 == 0:\n",
    "                progress_bar.set_postfix(loss=loss.item())\n",
    "    # save the model\n",
    "    with open(\".models/medmnist.ckpt\", \"wb\") as file:\n",
    "        torch.save(model.state_dict(), file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate the pre-trained model\n",
    "\n",
    "The pre-trained model performs poorly on the unseen drusen class, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model accuracy (all classes): 0.84\n",
      "Pre-trained model accuracy (excluding drusen): 0.95\n",
      "Pre-trained model accuracy (drusen class): 0.51\n"
     ]
    }
   ],
   "source": [
    "# evaluate the pre-trained model\n",
    "_, dataset_test_all = get_datasets()\n",
    "x, u = dataset_test_all.tensors\n",
    "u, x = u.to(device), x.to(device)\n",
    "output = torch.sigmoid(model(x))\n",
    "preds = (output > 0.5)\n",
    "accuracy = (preds == u).float().mean().item()\n",
    "print(f\"Pre-trained model accuracy (all classes): {accuracy:.2f}\")\n",
    "\n",
    "_, dataset_test_no_drusen = get_datasets(exclude_classes=[2])\n",
    "x, u = dataset_test_no_drusen.tensors\n",
    "u, x = u.to(device), x.to(device)\n",
    "output = torch.sigmoid(model(x))\n",
    "preds = (output > 0.5)\n",
    "accuracy = (preds == u).float().mean().item()\n",
    "print(f\"Pre-trained model accuracy (excluding drusen): {accuracy:.2f}\")\n",
    "\n",
    "_, dataset_test_drusen = get_datasets(exclude_classes=[0, 1, 3])\n",
    "x, u = dataset_test_drusen.tensors\n",
    "u, x = u.to(device), x.to(device)\n",
    "output = torch.sigmoid(model(x))\n",
    "preds = (output > 0.5)\n",
    "accuracy = (preds == u).float().mean().item()\n",
    "print(f\"Pre-trained model accuracy (drusen class): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fine-tune the model on the drusen class using AGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the AGT configuration\n",
    "batchsize = 5000\n",
    "nominal_config = AGTConfig(\n",
    "    fragsize=2000,\n",
    "    learning_rate=0.1,\n",
    "    n_epochs=3,\n",
    "    device=\"cuda:0\",\n",
    "    l2_reg=0.01,\n",
    "    k_private=10,\n",
    "    loss=\"binary_cross_entropy\",\n",
    "    log_level=\"INFO\",\n",
    "    lr_decay=2.0,\n",
    "    clip_gamma=1.0,\n",
    "    lr_min=0.001,\n",
    "    optimizer=\"SGDM\", # we'll use SGD with momentum\n",
    "    optimizer_kwargs={\"momentum\": 0.9, \"nesterov\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataloaders, train dataloader is a mix of drusen and the \"healthy\" class\n",
    "dataset_train, _ = get_datasets(exclude_classes=[0, 1], balanced=True)  # a mix of drusen (class 2) and normal (class 3)\n",
    "_, dataset_test_drusen = get_datasets(exclude_classes=[0, 1, 3])  # drusen only (class 2)\n",
    "torch.manual_seed(0)\n",
    "dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "dl_test_drusen = torch.utils.data.DataLoader(dataset_test_drusen, batch_size=batchsize, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are only going to fine-tune the dense layers of our model, so we'll handle that logic here\n",
    "# we'll form a separate bounded model for the convolutional layers and\n",
    "# use it as the `transform` argument of the bounded model of the dense layers\n",
    "conv_layers, dense_layers = model[0:5], model[5:]\n",
    "conv_bounded_model = IntervalBoundedModel(conv_layers, trainable=False)\n",
    "bounded_model = IntervalBoundedModel(dense_layers, trainable=True, transform=conv_bounded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [17:51:27] =================== Starting Privacy Certified Training ===================\n",
      "[AGT] [INFO    ] [17:51:27] Starting epoch 1\n",
      "[AGT] [INFO    ] [17:51:27] Batch 0. Loss (accuracy): 0.508 <= 0.508 <= 0.508\n",
      "[AGT] [INFO    ] [17:51:28] Batch 1. Loss (accuracy): 0.660 <= 0.712 <= 0.740\n",
      "[AGT] [INFO    ] [17:51:28] Batch 2. Loss (accuracy): 0.732 <= 0.796 <= 0.832\n",
      "[AGT] [INFO    ] [17:51:29] Starting epoch 2\n",
      "[AGT] [INFO    ] [17:51:29] Batch 3. Loss (accuracy): 0.748 <= 0.816 <= 0.868\n",
      "[AGT] [INFO    ] [17:51:30] Batch 4. Loss (accuracy): 0.760 <= 0.840 <= 0.908\n",
      "[AGT] [INFO    ] [17:51:31] Batch 5. Loss (accuracy): 0.740 <= 0.848 <= 0.928\n",
      "[AGT] [INFO    ] [17:51:32] Starting epoch 3\n",
      "[AGT] [INFO    ] [17:51:32] Batch 6. Loss (accuracy): 0.728 <= 0.864 <= 0.928\n",
      "[AGT] [INFO    ] [17:51:33] Batch 7. Loss (accuracy): 0.708 <= 0.872 <= 0.932\n",
      "[AGT] [INFO    ] [17:51:34] Batch 8. Loss (accuracy): 0.668 <= 0.872 <= 0.940\n",
      "[AGT] [INFO    ] [17:51:34] Final Eval. Loss (accuracy): 0.648 <= 0.872 <= 0.952\n",
      "[AGT] [INFO    ] [17:51:34] =================== Finished Privacy Certified Training ===================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<abstract_gradient_training.bounded_models.interval_bounded_model.IntervalBoundedModel at 0x707c64b0a5c0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fine-tune the model using abstract gradient training (keeping the convolutional layers fixed)\n",
    "agt.privacy_certified_training(bounded_model, nominal_config, dl_train, dl_test_drusen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Evaluate the fine-tuned model\n",
    "\n",
    "The fine-tuned model performs better on the drusen class (80%+ accuracy) while maintaining accuracy on the other classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model accuracy + certified bounds (all classes): 0.80 <= 0.86 <= 0.91\n",
      "Pre-trained model accuracy + certified bounds (excluding drusen): 0.84 <= 0.88 <= 0.92\n",
      "Pre-trained model accuracy + certified bounds (drusen class): 0.67 <= 0.80 <= 0.86\n"
     ]
    }
   ],
   "source": [
    "# evaluate the fine-tuned model\n",
    "_, dataset_test_all = get_datasets()\n",
    "accuracy = agt.test_metrics.test_accuracy(bounded_model, *dataset_test_all.tensors)\n",
    "print(f\"Pre-trained model accuracy + certified bounds (all classes): {accuracy[0]:.2f} <= {accuracy[1]:.2f} <= {accuracy[2]:.2f}\")\n",
    "\n",
    "_, dataset_test_no_drusen = get_datasets(exclude_classes=[2])\n",
    "accuracy = agt.test_metrics.test_accuracy(bounded_model, *dataset_test_no_drusen.tensors)\n",
    "print(f\"Pre-trained model accuracy + certified bounds (excluding drusen): {accuracy[0]:.2f} <= {accuracy[1]:.2f} <= {accuracy[2]:.2f}\")\n",
    "\n",
    "_, dataset_test_drusen = get_datasets(exclude_classes=[0, 1, 3])\n",
    "accuracy = agt.test_metrics.test_accuracy(bounded_model, *dataset_test_drusen.tensors)\n",
    "print(f\"Pre-trained model accuracy + certified bounds (drusen class): {accuracy[0]:.2f} <= {accuracy[1]:.2f} <= {accuracy[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Use the AGT certificates to make privacy-preserving predictions on the test dataset\n",
    "\n",
    "First, run AGT for a range of k_private values. Then, use the parameter bounds to compute the smooth sensitivity of the model on the test-set predictions. Finally, use the smooth sensitivity to calibrate the amount of noise to add to the predictions to ensure differential privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:36<00:00,  5.23s/it]\n"
     ]
    }
   ],
   "source": [
    "# to use privacy-safe certificates, we need to run AGT for a range of k_private values\n",
    "\n",
    "# we'll just pick a reasonable range of k_private values. adding more values will increase the runtime\n",
    "# but also result in tighter privacy results. even a few values are sufficient to demonstrate tighter privacy\n",
    "\n",
    "k_private_values = [1, 2, 5, 10, 20, 50, 100] \n",
    "privacy_bounded_models = {}\n",
    "config = copy.deepcopy(nominal_config)\n",
    "config.log_level = \"WARNING\"\n",
    "\n",
    "for k_private in tqdm.tqdm(k_private_values):\n",
    "    # update config\n",
    "    config.k_private = k_private\n",
    "    # form bounded model\n",
    "    conv_layers, dense_layers = model[0:5], model[5:]\n",
    "    conv_bounded_model = IntervalBoundedModel(conv_layers, trainable=False)\n",
    "    bounded_model = IntervalBoundedModel(dense_layers, trainable=True, transform=conv_bounded_model)\n",
    "    torch.manual_seed(0)\n",
    "    dl_train = torch.utils.data.DataLoader(dataset_train, batch_size=batchsize, shuffle=True)\n",
    "    # run AGT\n",
    "    agt.privacy_certified_training(bounded_model, config, dl_train, dl_test_drusen)\n",
    "    privacy_bounded_models[k_private] = bounded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy using global sensitivity: 0.65\n",
      "Accuracy using AGT smooth sensitivity bounds: 0.86\n"
     ]
    }
   ],
   "source": [
    "# make privacy-safe predictions using the global sensitivity\n",
    "epsilon = 1.0\n",
    "_, dataset_test_all = get_datasets()\n",
    "accuracy = agt.privacy_utils.noisy_test_accuracy(\n",
    "    bounded_model, *dataset_test_all.tensors, noise_level=1 / epsilon, noise_type=\"laplace\"\n",
    ")\n",
    "print(f\"Accuracy using global sensitivity: {accuracy:.2f}\")\n",
    "\n",
    "# make privacy-safe predictions using the smooth sensitivity bounds from AGT\n",
    "noise_level = agt.privacy_utils.get_calibrated_noise_level(\n",
    "    dataset_test_all.tensors[0], privacy_bounded_models, epsilon=epsilon, noise_type=\"cauchy\" \n",
    ")\n",
    "accuracy = agt.privacy_utils.noisy_test_accuracy(\n",
    "    bounded_model, *dataset_test_all.tensors, noise_level=noise_level, noise_type=\"cauchy\"\n",
    ")\n",
    "print(f\"Accuracy using AGT smooth sensitivity bounds: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

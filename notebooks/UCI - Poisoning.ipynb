{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poison Certified Training on UCI Datasets\n",
    "\n",
    "In this notebook, we'll train a simple neural network with 1 hidden layer on the UCI-houseelectric regression task. We'll use Abstract Gradient Training to compute bounds on the parameters of the model under a potential data poisoning attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x73d5535cd3f0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import abstract_gradient_training as agt\n",
    "from abstract_gradient_training.bounded_models import IntervalBoundedModel\n",
    "\n",
    "import uci_datasets  # python -m pip install git+https://github.com/treforevans/uci_datasets.git\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "houseelectric dataset, N=2049280, d=11\n"
     ]
    }
   ],
   "source": [
    "data = uci_datasets.Dataset(\"houseelectric\")\n",
    "x_train, y_train, x_test, y_test = data.get_split(split=0)\n",
    "\n",
    "# Normalise the features and labels\n",
    "x_train_mu, x_train_std = x_train.mean(axis=0), x_train.std(axis=0)\n",
    "x_train = (x_train - x_train_mu) / x_train_std\n",
    "x_test = (x_test - x_train_mu) / x_train_std\n",
    "y_train_min, y_train_range = y_train.min(axis=0), y_train.max(axis=0) - y_train.min(axis=0)\n",
    "y_train = (y_train - y_train_min) / y_train_range\n",
    "y_test = (y_test - y_train_min) / y_train_range\n",
    "\n",
    "# Form datasets and dataloaders\n",
    "train_data = torch.utils.data.TensorDataset(torch.from_numpy(x_train).float(), torch.from_numpy(y_train).float())\n",
    "test_data = torch.utils.data.TensorDataset(torch.from_numpy(x_test).float(), torch.from_numpy(y_test).float())\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20000, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure the poisoning attack and training parameters\n",
    "\n",
    "We'll bound the following feature-poisoning attack: in each batch, up to `k_poison=200` data points can be poisoned\n",
    "by up to `epsilon=0.01` in the $\\ell_\\infty$-norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the training parameters\n",
    "config = agt.AGTConfig(\n",
    "    fragsize=20000,\n",
    "    learning_rate=0.005,\n",
    "    epsilon=0.01,\n",
    "    k_poison=200,\n",
    "    n_epochs=1,\n",
    "    device=\"cuda:1\",\n",
    "    loss=\"mse\",\n",
    "    log_level=\"DEBUG\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the nn model\n",
    "model = torch.nn.Sequential(torch.nn.Linear(11, 64), torch.nn.ReLU(), torch.nn.Linear(64, 1)).to(config.device)\n",
    "# we'll wrap this model in a bounded version that can compute the bounds on the outputs and gradients required for AGT\n",
    "bounded_model = IntervalBoundedModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model using AGT\n",
    "\n",
    "At each iteration, AGT will report the current MSE plus its certified bounds. Logging can be controlled via the `log_level` parameter in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [INFO    ] [12:22:27] =================== Starting Poison Certified Training ===================\n",
      "[AGT] [DEBUG   ] [12:22:27] \tOptimizer params: n_epochs=1, learning_rate=0.005, l1_reg=0.0, l2_reg=0.0\n",
      "[AGT] [DEBUG   ] [12:22:27] \tLearning rate schedule: lr_decay=0.0, lr_min=0.0\n",
      "[AGT] [DEBUG   ] [12:22:27] \tGradient clipping: gamma=inf, method=clamp\n",
      "[AGT] [DEBUG   ] [12:22:27] \tGradient noise: type=gaussian, multiplier=0\n",
      "[AGT] [DEBUG   ] [12:22:27] \tAdversary feature-space budget: epsilon=0.01, k_poison=200\n",
      "[AGT] [DEBUG   ] [12:22:27] \tAdversary label-space budget: label_epsilon=0, label_k_poison=0, poison_target_idx=-1\n",
      "[AGT] [DEBUG   ] [12:22:27] \tPaired poisoning: False\n",
      "[AGT] [INFO    ] [12:22:27] Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[AGT] [DEBUG   ] [12:22:28] Initialising dataloader batchsize to 20000\n",
      "[AGT] [INFO    ] [12:22:28] Batch 1. Loss (mse): 0.201 <= 0.201 <= 0.201\n",
      "[AGT] [INFO    ] [12:22:28] Batch 2. Loss (mse): 0.182 <= 0.182 <= 0.182\n",
      "[AGT] [INFO    ] [12:22:28] Batch 3. Loss (mse): 0.165 <= 0.166 <= 0.166\n",
      "[AGT] [INFO    ] [12:22:29] Batch 4. Loss (mse): 0.153 <= 0.154 <= 0.154\n",
      "[AGT] [INFO    ] [12:22:29] Batch 5. Loss (mse): 0.132 <= 0.132 <= 0.133\n",
      "[AGT] [INFO    ] [12:22:29] Batch 6. Loss (mse): 0.130 <= 0.131 <= 0.131\n",
      "[AGT] [INFO    ] [12:22:29] Batch 7. Loss (mse): 0.116 <= 0.117 <= 0.118\n",
      "[AGT] [INFO    ] [12:22:29] Batch 8. Loss (mse): 0.107 <= 0.107 <= 0.108\n",
      "[AGT] [INFO    ] [12:22:29] Batch 9. Loss (mse): 0.094 <= 0.095 <= 0.096\n",
      "[AGT] [INFO    ] [12:22:30] Batch 10. Loss (mse): 0.092 <= 0.093 <= 0.094\n",
      "[AGT] [INFO    ] [12:22:30] Batch 11. Loss (mse): 0.087 <= 0.088 <= 0.089\n",
      "[AGT] [INFO    ] [12:22:30] Batch 12. Loss (mse): 0.082 <= 0.084 <= 0.085\n",
      "[AGT] [INFO    ] [12:22:30] Batch 13. Loss (mse): 0.075 <= 0.076 <= 0.078\n",
      "[AGT] [INFO    ] [12:22:30] Batch 14. Loss (mse): 0.065 <= 0.066 <= 0.068\n",
      "[AGT] [INFO    ] [12:22:31] Batch 15. Loss (mse): 0.065 <= 0.066 <= 0.067\n",
      "[AGT] [INFO    ] [12:22:31] Batch 16. Loss (mse): 0.065 <= 0.067 <= 0.068\n",
      "[AGT] [INFO    ] [12:22:31] Batch 17. Loss (mse): 0.058 <= 0.060 <= 0.061\n",
      "[AGT] [INFO    ] [12:22:31] Batch 18. Loss (mse): 0.053 <= 0.054 <= 0.056\n",
      "[AGT] [INFO    ] [12:22:31] Batch 19. Loss (mse): 0.051 <= 0.053 <= 0.055\n",
      "[AGT] [INFO    ] [12:22:32] Batch 20. Loss (mse): 0.051 <= 0.053 <= 0.055\n",
      "[AGT] [INFO    ] [12:22:32] Batch 21. Loss (mse): 0.049 <= 0.051 <= 0.053\n",
      "[AGT] [INFO    ] [12:22:32] Batch 22. Loss (mse): 0.049 <= 0.051 <= 0.053\n",
      "[AGT] [INFO    ] [12:22:32] Batch 23. Loss (mse): 0.046 <= 0.048 <= 0.050\n",
      "[AGT] [INFO    ] [12:22:32] Batch 24. Loss (mse): 0.047 <= 0.049 <= 0.052\n",
      "[AGT] [INFO    ] [12:22:32] Batch 25. Loss (mse): 0.042 <= 0.045 <= 0.047\n",
      "[AGT] [INFO    ] [12:22:33] Batch 26. Loss (mse): 0.042 <= 0.045 <= 0.048\n",
      "[AGT] [INFO    ] [12:22:33] Batch 27. Loss (mse): 0.038 <= 0.041 <= 0.044\n",
      "[AGT] [INFO    ] [12:22:33] Batch 28. Loss (mse): 0.039 <= 0.042 <= 0.045\n",
      "[AGT] [INFO    ] [12:22:33] Batch 29. Loss (mse): 0.037 <= 0.041 <= 0.045\n",
      "[AGT] [INFO    ] [12:22:33] Batch 30. Loss (mse): 0.036 <= 0.040 <= 0.044\n",
      "[AGT] [INFO    ] [12:22:34] Batch 31. Loss (mse): 0.035 <= 0.039 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:34] Batch 32. Loss (mse): 0.034 <= 0.038 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:34] Batch 33. Loss (mse): 0.034 <= 0.039 <= 0.044\n",
      "[AGT] [INFO    ] [12:22:34] Batch 34. Loss (mse): 0.035 <= 0.040 <= 0.046\n",
      "[AGT] [INFO    ] [12:22:34] Batch 35. Loss (mse): 0.032 <= 0.037 <= 0.042\n",
      "[AGT] [INFO    ] [12:22:35] Batch 36. Loss (mse): 0.028 <= 0.033 <= 0.039\n",
      "[AGT] [INFO    ] [12:22:35] Batch 37. Loss (mse): 0.031 <= 0.037 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:35] Batch 38. Loss (mse): 0.030 <= 0.036 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:35] Batch 39. Loss (mse): 0.028 <= 0.034 <= 0.041\n",
      "[AGT] [INFO    ] [12:22:35] Batch 40. Loss (mse): 0.028 <= 0.034 <= 0.042\n",
      "[AGT] [INFO    ] [12:22:35] Batch 41. Loss (mse): 0.027 <= 0.034 <= 0.042\n",
      "[AGT] [INFO    ] [12:22:36] Batch 42. Loss (mse): 0.027 <= 0.034 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:36] Batch 43. Loss (mse): 0.025 <= 0.033 <= 0.043\n",
      "[AGT] [INFO    ] [12:22:36] Batch 44. Loss (mse): 0.026 <= 0.034 <= 0.045\n",
      "[AGT] [INFO    ] [12:22:36] Batch 45. Loss (mse): 0.025 <= 0.034 <= 0.046\n",
      "[AGT] [INFO    ] [12:22:36] Batch 46. Loss (mse): 0.023 <= 0.032 <= 0.044\n",
      "[AGT] [INFO    ] [12:22:37] Batch 47. Loss (mse): 0.023 <= 0.033 <= 0.048\n",
      "[AGT] [INFO    ] [12:22:37] Batch 48. Loss (mse): 0.022 <= 0.033 <= 0.047\n",
      "[AGT] [INFO    ] [12:22:37] Batch 49. Loss (mse): 0.020 <= 0.031 <= 0.046\n",
      "[AGT] [INFO    ] [12:22:37] Batch 50. Loss (mse): 0.021 <= 0.032 <= 0.049\n",
      "[AGT] [INFO    ] [12:22:37] Batch 51. Loss (mse): 0.022 <= 0.035 <= 0.054\n",
      "[AGT] [INFO    ] [12:22:38] Batch 52. Loss (mse): 0.019 <= 0.032 <= 0.050\n",
      "[AGT] [INFO    ] [12:22:38] Batch 53. Loss (mse): 0.018 <= 0.031 <= 0.053\n",
      "[AGT] [INFO    ] [12:22:38] Batch 54. Loss (mse): 0.018 <= 0.033 <= 0.057\n",
      "[AGT] [INFO    ] [12:22:38] Batch 55. Loss (mse): 0.018 <= 0.034 <= 0.059\n",
      "[AGT] [INFO    ] [12:22:38] Batch 56. Loss (mse): 0.014 <= 0.029 <= 0.055\n",
      "[AGT] [INFO    ] [12:22:38] Batch 57. Loss (mse): 0.016 <= 0.032 <= 0.061\n",
      "[AGT] [INFO    ] [12:22:39] Batch 58. Loss (mse): 0.014 <= 0.031 <= 0.064\n",
      "[AGT] [INFO    ] [12:22:39] Batch 59. Loss (mse): 0.013 <= 0.030 <= 0.062\n",
      "[AGT] [INFO    ] [12:22:39] Batch 60. Loss (mse): 0.013 <= 0.030 <= 0.067\n",
      "[AGT] [INFO    ] [12:22:39] Batch 61. Loss (mse): 0.011 <= 0.031 <= 0.073\n",
      "[AGT] [INFO    ] [12:22:39] Batch 62. Loss (mse): 0.009 <= 0.028 <= 0.071\n",
      "[AGT] [INFO    ] [12:22:40] Batch 63. Loss (mse): 0.009 <= 0.029 <= 0.077\n",
      "[AGT] [INFO    ] [12:22:40] Batch 64. Loss (mse): 0.008 <= 0.029 <= 0.079\n",
      "[AGT] [INFO    ] [12:22:40] Batch 65. Loss (mse): 0.008 <= 0.029 <= 0.086\n",
      "[AGT] [INFO    ] [12:22:40] Batch 66. Loss (mse): 0.008 <= 0.032 <= 0.096\n",
      "[AGT] [INFO    ] [12:22:40] Batch 67. Loss (mse): 0.007 <= 0.029 <= 0.096\n",
      "[AGT] [INFO    ] [12:22:41] Batch 68. Loss (mse): 0.006 <= 0.030 <= 0.107\n",
      "[AGT] [INFO    ] [12:22:41] Batch 69. Loss (mse): 0.005 <= 0.029 <= 0.112\n",
      "[AGT] [INFO    ] [12:22:41] Batch 70. Loss (mse): 0.005 <= 0.030 <= 0.121\n",
      "[AGT] [INFO    ] [12:22:41] Batch 71. Loss (mse): 0.004 <= 0.030 <= 0.130\n",
      "[AGT] [INFO    ] [12:22:41] Batch 72. Loss (mse): 0.003 <= 0.030 <= 0.144\n",
      "[AGT] [INFO    ] [12:22:42] Batch 73. Loss (mse): 0.003 <= 0.027 <= 0.149\n",
      "[AGT] [INFO    ] [12:22:42] Batch 74. Loss (mse): 0.002 <= 0.026 <= 0.162\n",
      "[AGT] [INFO    ] [12:22:42] Batch 75. Loss (mse): 0.001 <= 0.029 <= 0.179\n",
      "[AGT] [INFO    ] [12:22:42] Batch 76. Loss (mse): 0.002 <= 0.029 <= 0.197\n",
      "[AGT] [INFO    ] [12:22:42] Batch 77. Loss (mse): 0.002 <= 0.030 <= 0.215\n",
      "[AGT] [INFO    ] [12:22:42] Batch 78. Loss (mse): 0.001 <= 0.027 <= 0.230\n",
      "[AGT] [INFO    ] [12:22:43] Batch 79. Loss (mse): 0.001 <= 0.027 <= 0.252\n",
      "[AGT] [INFO    ] [12:22:43] Batch 80. Loss (mse): 0.000 <= 0.027 <= 0.287\n",
      "[AGT] [INFO    ] [12:22:43] Batch 81. Loss (mse): 0.000 <= 0.028 <= 0.339\n",
      "[AGT] [INFO    ] [12:22:43] Batch 82. Loss (mse): 0.000 <= 0.029 <= 0.362\n",
      "[AGT] [INFO    ] [12:22:43] Batch 83. Loss (mse): 0.000 <= 0.028 <= 0.411\n",
      "[AGT] [INFO    ] [12:22:44] Batch 84. Loss (mse): 0.000 <= 0.027 <= 0.451\n",
      "[AGT] [INFO    ] [12:22:44] Batch 85. Loss (mse): 0.000 <= 0.026 <= 0.500\n",
      "[AGT] [INFO    ] [12:22:44] Batch 86. Loss (mse): 0.000 <= 0.029 <= 0.571\n",
      "[AGT] [INFO    ] [12:22:44] Batch 87. Loss (mse): 0.000 <= 0.026 <= 0.649\n",
      "[AGT] [INFO    ] [12:22:44] Batch 88. Loss (mse): 0.000 <= 0.027 <= 0.770\n",
      "[AGT] [INFO    ] [12:22:45] Batch 89. Loss (mse): 0.000 <= 0.026 <= 0.811\n",
      "[AGT] [INFO    ] [12:22:45] Batch 90. Loss (mse): 0.000 <= 0.026 <= 0.959\n",
      "[AGT] [INFO    ] [12:22:45] Batch 91. Loss (mse): 0.000 <= 0.026 <= 1.100\n",
      "[AGT] [INFO    ] [12:22:45] Batch 92. Loss (mse): 0.000 <= 0.029 <= 1.290\n",
      "[AGT] [DEBUG   ] [12:22:45] Skipping batch 93 in epoch 1 (expected batchsize 20000, got 4352)\n",
      "[AGT] [INFO    ] [12:22:45] Final Eval. Loss (mse): 0.000 <= 0.025 <= 1.522\n",
      "[AGT] [INFO    ] [12:22:45] =================== Finished Poison Certified Training ===================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "IntervalBoundedModel(\n",
       "\tself.modules=[\n",
       "\t\tLinear(in_features=11, out_features=64, bias=True)\n",
       "\t\tReLU()\n",
       "\t\tLinear(in_features=64, out_features=1, bias=True)\n",
       "\t],\n",
       "\tself.interval_matmul='rump',\n",
       "\tself.trainable=True,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agt. poison_certified_training(bounded_model, config, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compute bounds on the final MSE of the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: nominal = 0.02631, certified upper bound = 1.473, certified lower bound = 0\n"
     ]
    }
   ],
   "source": [
    "# evaluate the trained model\n",
    "test_batch, test_labels = next(iter(test_loader))\n",
    "mse = agt.test_metrics.test_mse(bounded_model, test_batch, test_labels)\n",
    "print(f\"Test MSE: nominal = {mse[1]:.4g}, certified upper bound = {mse[0]:.4g}, certified lower bound = {mse[2]:.4g}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
